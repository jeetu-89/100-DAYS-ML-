# 100-DAYS-ML-

## Day 1
### PYTORCH -
1. Training Pipeline: Model, Loss, and Optimizer
2. Automatic Differentiation
3. 

### OOPS IN PYTHON -
1. Static Methods
2. Instance variables vs Class variables
3. Class Methods
4. Class Methods as Alternative Constructors
5. dir, __dict__ and help method
6. super keyword
7. Magic/Dunder Methods
8. Method Overriding

## Day 2
### PYTORCH 
1. nn.Sequential
2. nn.Module
3. Creating Custom Layers
4. Dataset and Datastores

## Day 3
### HUGGING FACE 
#### 1. Transformer Model
1. Introduction
2. Natural Language Processing
3. Transformers, what can they do?
4. How do Transformers work?
5. Encoder models
6. Decoder models
7. Sequence-to-sequence models sequence-to-sequence-models
8. Bias and limitations
9. Summary
### BLOGS
Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) - https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

## Day 4
### HUGGING FACE
#### 2. USING ðŸ¤— TRANSFORMERS
1. Inroduction
2. Behind the pipeline
3. Models

## Day 5
### HUGGING FACE
#### 2. USING ðŸ¤— TRANSFORMERS
4. Tokenizers
5. Handling multiple sequences
6. Putting it all together
7. End-of-chapter quiz
### 3. FINE-TUNING A PRETRAINED MODEL
1. Introduction
2. Processing the data

## Day 6
### HUGGING FACE
### 3. FINE-TUNING A PRETRAINED MODEL
3. Fine-tuning a model with the Trainer API
4. A full training
   
## Day 7
### HUGGING FACE
#### 5. THE ðŸ¤— DATASETS LIBRARY
1. Introduction
2. What if my dataset isnâ€™t on the Hub?

### Installation of cuda and jupyter notebook in WSL and creating local environment

## Day 8
### HUGGING FACE
#### 5. THE ðŸ¤— DATASETS LIBRARY
3. Time to slice and dice

## Day 9
### HUGGING FACE
#### 5. THE ðŸ¤— DATASETS LIBRARY
3. Time to slice and dice

#### BLOGS
1. For Loop vs List Comprehension vs Map Function : https://www.linkedin.com/pulse/loop-vs-list-comprehension-map-function-akhilesh-singh#:~:text=specific%20use%20case.-,List%20comprehensions%20are%20often%20faster%20than%20loops%20because%20they%20use,lead%20to%20more%20efficient%20code.

## Day 10
### HUGGING FACE
#### 6. THE ðŸ¤— TOKENIZERS LIBRARY
1. Introduction
2. Training a new tokenizer from an old one
   - Assembling a corpus
   - Training a new tokenizer
   - Saving the tokenizer
## Day 10
### HUGGING FACE
#### 6. THE ðŸ¤— TOKENIZERS LIBRARY
3. Fast tokenizersâ€™ special powers
   - Batch encoding
   - Inside the token-classification pipeline
     . Getting the base results with the pipeline
     . 
