# 100-DAYS-ML-

## Day 1
### PYTORCH -
1. Training Pipeline: Model, Loss, and Optimizer
2. Automatic Differentiation
3. 

### OOPS IN PYTHON -
1. Static Methods
2. Instance variables vs Class variables
3. Class Methods
4. Class Methods as Alternative Constructors
5. dir, __dict__ and help method
6. super keyword
7. Magic/Dunder Methods
8. Method Overriding

## Day 2
### PYTORCH 
1. nn.Sequential
2. nn.Module
3. Creating Custom Layers
4. Dataset and Datastores

## Day 3
### HUGGING FACE 
#### 1. Transformer Model
1. Introduction
2. Natural Language Processing
3. Transformers, what can they do?
4. How do Transformers work?
5. Encoder models
6. Decoder models
7. Sequence-to-sequence models sequence-to-sequence-models
8. Bias and limitations
9. Summary
### BLOGS
Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) - https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

## Day 4
### HUGGING FACE
#### 2. USING ðŸ¤— TRANSFORMERS
1. Inroduction
2. Behind the pipeline
3. Models

## Day 5
### HUGGING FACE
#### 2. USING ðŸ¤— TRANSFORMERS
4. Tokenizers
5. Handling multiple sequences
6. Putting it all together
7. End-of-chapter quiz
### 3. FINE-TUNING A PRETRAINED MODEL
1. Introduction
2. Processing the data

## Day 6
### HUGGING FACE
### 3. FINE-TUNING A PRETRAINED MODEL
3. Fine-tuning a model with the Trainer API
4. A full training
   
## Day 7
### HUGGING FACE
#### 5. THE ðŸ¤— DATASETS LIBRARY
1. Introduction
2. What if my dataset isnâ€™t on the Hub?

### Installation of cuda and jupyter notebook in WSL 
